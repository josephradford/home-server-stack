groups:
  # System and Container Health Monitoring
  # Consolidated alerts for system resources and container availability
  - name: system-alerts
    rules:
      # General service health monitoring using Prometheus 'up' metric
      # Triggers when any monitored endpoint fails to respond to health checks
      # 30s threshold: Short enough to detect failures quickly, long enough to avoid network glitches
      - alert: ServiceDown
        expr: up == 0
        for: 30s
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 30 seconds"

      # System-wide CPU usage monitoring
      # Calculates average CPU usage across all cores by measuring non-idle time
      # 85% threshold: Leaves 15% headroom for burst activity, prevents performance degradation
      # 5m duration: Filters out brief spikes from legitimate workloads (n8n workflows)
      - alert: HighCPUUsage
        expr: 100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 5m
        labels:
          severity: critical
          category: resources
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 85% for more than 5 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # System memory exhaustion monitoring using MemAvailable (includes cache/buffers)
      # MemAvailable is more accurate than simple used/total as it accounts for reclaimable memory
      # 90% threshold: Critical level - risk of OOM killer activation
      # 3m duration: Shorter than CPU as memory exhaustion leads to immediate failures
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 3m
        labels:
          severity: critical
          category: resources
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 90% for more than 3 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # Root filesystem capacity monitoring
      # Monitors only root mountpoint (/) to avoid false alerts from tmpfs, devtmpfs, etc.
      # 90% threshold: Critical level - services may fail when disk is full
      # 5m duration: Allows time for transient large file operations to complete
      - alert: DiskSpaceLow
        expr: 100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100) > 90
        for: 5m
        labels:
          severity: critical
          category: resources
        annotations:
          summary: "Disk space critically low"
          description: "Disk space usage on {{ $labels.instance }} is above 90% for more than 5 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # Disk I/O saturation monitoring
      # High I/O wait indicates disk bottleneck or potential hardware issues
      # >0.5 threshold: More than 50% of time spent in I/O operations indicates saturation
      # 10m duration: Tolerates brief I/O spikes from backups and large file ops
      - alert: HighDiskIOWait
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.5
        for: 10m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "High disk I/O wait time"
          description: "Disk I/O wait time is high on {{ $labels.instance }} for more than 10 minutes"

      # Network traffic anomaly detection
      # Monitors receive bandwidth on physical interfaces (excludes loopback and docker virtual interfaces)
      # >100MB/s threshold: Unusually high for typical home server traffic, may indicate abuse or attack
      # 10m duration: Allows for legitimate large transfers (backups)
      - alert: HighNetworkTraffic
        expr: rate(node_network_receive_bytes_total{device!~"lo|docker.*"}[5m]) > 100000000
        for: 10m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "High network traffic detected"
          description: "Network interface {{ $labels.device }} on {{ $labels.instance }} is receiving high traffic for more than 10 minutes"

      # System load average monitoring (normalized by CPU count)
      # Load average includes processes waiting for CPU and I/O
      # >2x CPU count threshold: Load of 2.0 per CPU indicates significant queuing/contention
      # 10m duration: Uses 15-min load avg, sustained high load indicates capacity issue
      - alert: SystemLoadHigh
        expr: node_load15 / count(node_cpu_seconds_total{mode="idle"}) without (cpu, mode) > 2
        for: 10m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "System load is high"
          description: "15-minute load average on {{ $labels.instance }} is above 2x CPU count for more than 10 minutes"

      # Critical container availability monitoring via cAdvisor
      # Checks when cAdvisor last saw critical containers (adguard, n8n)
      # 60s threshold: Container not seen by cAdvisor for over a minute indicates it's stopped
      # for: 0m - Immediate alert as these are core services requiring high availability
      - alert: ContainerDown
        expr: time() - container_last_seen{name=~"adguard|n8n"} > 60
        for: 0m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Container {{ $labels.name }} is down"
          description: "Container {{ $labels.name }} has not been seen for more than 1 minute"

      # Per-container CPU usage monitoring
      # Monitors individual container CPU consumption using cAdvisor metrics
      # 80% threshold: Warning level for single container (lower than system-wide 85%)
      # 10m duration: Long window to tolerate n8n batch jobs
      - alert: ContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total{name=~".+"}[5m]) * 100 > 80
        for: 10m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "Container {{ $labels.name }} high CPU usage"
          description: "Container {{ $labels.name }} CPU usage is above 80% for more than 10 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # Per-container memory usage against configured limits
      # Only fires for containers with explicit memory limits set
      # 85% threshold: Warning before hitting limit and potential OOM kill
      # 10m duration: Allows for temporary memory spikes
      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name=~".+"} / container_spec_memory_limit_bytes{name=~".+"}) * 100 > 85
        for: 10m
        labels:
          severity: warning
          category: resources
        annotations:
          summary: "Container {{ $labels.name }} high memory usage"
          description: "Container {{ $labels.name }} memory usage is above 85% for more than 10 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # Container crash/restart loop detection
      # Tracks container start events - multiple starts indicate restart loop
      # >3 restarts in 1h threshold: Indicates persistent failure (crash loop)
      # for: 0m - Immediate alert as restart loops indicate critical configuration/bug issues
      - alert: ContainerRestartLoop
        expr: increase(container_start_time_seconds{name=~".+"}[1h]) > 3
        for: 0m
        labels:
          severity: warning
          category: reliability
        annotations:
          summary: "Container {{ $labels.name }} restarting frequently"
          description: "Container {{ $labels.name }} has restarted {{ printf \"%.0f\" $value }} times in the last hour"

  # Service-Specific Health Monitoring
  # Monitors critical application services
  - name: service-specific-alerts
    rules:
      # Monitoring infrastructure health - critical for observability
      # Alerts when core monitoring components fail (Prometheus, node-exporter, cAdvisor)
      # 1m threshold: Quick detection as these failures blind us to system state
      # More urgent than general ServiceDown as it affects all monitoring
      - alert: PrometheusTargetDown
        expr: up{job=~"prometheus|node-exporter|cadvisor"} == 0
        for: 1m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Monitoring target {{ $labels.job }} is down"
          description: "{{ $labels.job }} monitoring target on {{ $labels.instance }} has been down for more than 1 minute"

      # AdGuard Home DNS and ad-blocking service availability
      # Critical as network clients depend on this for DNS resolution
      # 1m threshold: DNS failures impact all network operations immediately
      - alert: AdGuardDown
        expr: up{job="adguard"} == 0
        for: 1m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "AdGuard DNS service is down"
          description: "AdGuard service is not responding for more than 1 minute"

      # n8n workflow automation platform availability
      # Critical as automated workflows and integrations depend on this service
      # Monitors container health via cAdvisor
      # 2m threshold: Slightly longer than DNS as brief restarts for config changes are acceptable
      - alert: N8nDown
        expr: absent(container_last_seen{name="n8n"}) or (time() - container_last_seen{name="n8n"} > 120)
        for: 2m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "n8n automation service is down"
          description: "n8n container has been down for more than 2 minutes"

  # VPN Security Monitoring
  # Monitors WireGuard VPN as the primary security boundary
  # Critical for maintaining secure remote access
  - name: vpn_alerts
    interval: 30s
    rules:
      # Monitor WireGuard service availability
      # This is critical as VPN is the primary remote access method
      - alert: WireGuardContainerDown
        expr: absent(container_last_seen{name="wireguard"}) or (time() - container_last_seen{name="wireguard"} > 120)
        for: 2m
        labels:
          severity: critical
          category: availability
          service: wireguard
        annotations:
          summary: "WireGuard VPN container is down"
          description: "WireGuard container has been down for more than 2 minutes. All remote VPN access is unavailable."

      # Monitor for excessive connection attempts (possible attack)
      # High rate of failed handshakes could indicate brute force or scanning
      - alert: WireGuardExcessiveConnectionAttempts
        expr: rate(container_network_receive_packets_total{name="wireguard"}[5m]) > 1000
        for: 3m
        labels:
          severity: warning
          category: security
          service: wireguard
        annotations:
          summary: "Unusual WireGuard connection activity"
          description: "High rate of network packets to WireGuard ({{ $value }} packets/sec). Possible port scanning or attack."

      # Monitor container restarts (could indicate crashes or issues)
      - alert: WireGuardFrequentRestarts
        expr: rate(container_start_time_seconds{name="wireguard"}[15m]) > 0.1
        for: 5m
        labels:
          severity: warning
          category: reliability
          service: wireguard
        annotations:
          summary: "WireGuard container restarting frequently"
          description: "WireGuard has restarted {{ $value }} times in 15 minutes. Check logs for errors."

      # Monitor container health status
      - alert: WireGuardUnhealthy
        expr: container_health_status{name="wireguard",status!="healthy"} == 1
        for: 5m
        labels:
          severity: warning
          category: reliability
          service: wireguard
        annotations:
          summary: "WireGuard container health check failing"
          description: "WireGuard health check has been failing for 5+ minutes."

  # Security Monitoring
  # Monitors for suspicious activity and potential attacks
  - name: security-alerts
    interval: 30s
    rules:
      # Monitor for high rate of webhook requests (potential DDoS or abuse)
      # Webhooks should have legitimate, predictable traffic patterns
      - alert: HighWebhookRate
        expr: rate(traefik_service_requests_total{service=~".*webhook.*"}[5m]) > 1
        for: 2m
        labels:
          severity: warning
          category: security
        annotations:
          summary: "High webhook request rate detected"
          description: "Webhook endpoint receiving {{ printf \"%.2f\" $value }} requests per second - possible abuse or DDoS"

      # Monitor for authentication failures (potential brute force)
      # Multiple 401 errors indicate password guessing attempts
      - alert: TooMany401Errors
        expr: rate(traefik_service_requests_total{code="401"}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
          category: security
        annotations:
          summary: "Authentication failures detected"
          description: "Multiple 401 errors ({{ printf \"%.2f\" $value }}/sec) - possible brute force attack"

      # Monitor for scanning activity (404 errors for non-existent paths)
      # High 404 rate indicates automated scanning for vulnerabilities
      - alert: TooMany404Errors
        expr: rate(traefik_service_requests_total{code="404"}[5m]) > 0.5
        for: 2m
        labels:
          severity: warning
          category: security
        annotations:
          summary: "Scanning activity detected"
          description: "High rate of 404 errors ({{ printf \"%.2f\" $value }}/sec) - possible vulnerability scanning"

      # Monitor for rate limit hits (429 Too Many Requests)
      # Indicates a client is exceeding configured limits
      - alert: RateLimitHit
        expr: rate(traefik_service_requests_total{code="429"}[5m]) > 0.1
        for: 1m
        labels:
          severity: info
          category: security
        annotations:
          summary: "Rate limit being enforced"
          description: "Client hitting rate limits ({{ printf \"%.2f\" $value }}/sec) - legitimate spike or abuse?"

      # Monitor for server errors that might indicate attacks
      # 500-level errors could indicate exploitation attempts
      - alert: HighServerErrors
        expr: rate(traefik_service_requests_total{code=~"5.."}[5m]) > 0.2
        for: 3m
        labels:
          severity: warning
          category: security
        annotations:
          summary: "High rate of server errors"
          description: "Server returning {{ printf \"%.2f\" $value }} errors/sec - check for attacks or misconfigurations"

      # Monitor Traefik service availability
      # Traefik is critical - its failure exposes or blocks all services
      - alert: TraefikDown
        expr: up{job="traefik"} == 0
        for: 1m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Traefik reverse proxy is down"
          description: "Traefik has been down for 1 minute - all web services are inaccessible"

      # Monitor fail2ban container availability
      # Fail2ban is our automated defense against attacks
      - alert: Fail2banDown
        expr: absent(container_last_seen{name="fail2ban"}) or (time() - container_last_seen{name="fail2ban"} > 120)
        for: 2m
        labels:
          severity: warning
          category: security
        annotations:
          summary: "Fail2ban protection is down"
          description: "Fail2ban container has been down for 2+ minutes - no automated attack blocking"

