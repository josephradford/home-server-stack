groups:
  - name: critical-alerts
    rules:
      # General service health monitoring using Prometheus 'up' metric
      # Triggers when any monitored endpoint fails to respond to health checks
      # 30s threshold: Short enough to detect failures quickly, long enough to avoid network glitches
      - alert: ServiceDown
        expr: up == 0
        for: 30s
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 30 seconds"

      # System-wide CPU usage monitoring
      # Calculates average CPU usage across all cores by measuring non-idle time
      # 85% threshold: Leaves 15% headroom for burst activity, prevents performance degradation
      # 5m duration: Filters out brief spikes from legitimate workloads (Ollama inference, n8n workflows)
      - alert: HighCPUUsage
        expr: 100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 85% for more than 5 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # System memory exhaustion monitoring using MemAvailable (includes cache/buffers)
      # MemAvailable is more accurate than simple used/total as it accounts for reclaimable memory
      # 90% threshold: Critical level - risk of OOM killer activation
      # 3m duration: Shorter than CPU as memory exhaustion leads to immediate failures
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 3m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 90% for more than 3 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # Root filesystem capacity monitoring
      # Monitors only root mountpoint (/) to avoid false alerts from tmpfs, devtmpfs, etc.
      # 90% threshold: Critical level - services may fail when disk is full
      # 5m duration: Allows time for transient large file operations (model downloads) to complete
      - alert: DiskSpaceLow
        expr: 100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100) > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Disk space critically low"
          description: "Disk space usage on {{ $labels.instance }} is above 90% for more than 5 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # Critical container availability monitoring via cAdvisor
      # Checks when cAdvisor last saw critical containers (adguard, n8n, ollama)
      # 60s threshold: Container not seen by cAdvisor for over a minute indicates it's stopped
      # for: 0m - Immediate alert as these are core services requiring high availability
      - alert: ContainerDown
        expr: time() - container_last_seen{name=~"adguard|n8n|ollama"} > 60
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Container {{ $labels.name }} is down"
          description: "Container {{ $labels.name }} has not been seen for more than 1 minute"

      # Per-container CPU usage monitoring
      # Monitors individual container CPU consumption using cAdvisor metrics
      # 80% threshold: Warning level for single container (lower than system-wide 85%)
      # 10m duration: Long window to tolerate Ollama inference and n8n batch jobs
      - alert: ContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total{name=~".+"}[5m]) * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} high CPU usage"
          description: "Container {{ $labels.name }} CPU usage is above 80% for more than 10 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # Per-container memory usage against configured limits
      # Only fires for containers with explicit memory limits set
      # 85% threshold: Warning before hitting limit and potential OOM kill
      # 10m duration: Allows for temporary memory spikes during model loading
      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name=~".+"} / container_spec_memory_limit_bytes{name=~".+"}) * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} high memory usage"
          description: "Container {{ $labels.name }} memory usage is above 85% for more than 10 minutes (current value: {{ printf \"%.2f\" $value }}%)"

      # Container crash/restart loop detection
      # Tracks container start events - multiple starts indicate restart loop
      # >3 restarts in 1h threshold: Indicates persistent failure (crash loop)
      # for: 0m - Immediate alert as restart loops indicate critical configuration/bug issues
      - alert: ContainerRestartLoop
        expr: increase(container_start_time_seconds{name=~".+"}[1h]) > 3
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} restarting frequently"
          description: "Container {{ $labels.name }} has restarted {{ printf \"%.0f\" $value }} times in the last hour"

  - name: service-specific-alerts
    rules:
      # Monitoring infrastructure health - critical for observability
      # Alerts when core monitoring components fail (Prometheus, node-exporter, cAdvisor)
      # 1m threshold: Quick detection as these failures blind us to system state
      # More urgent than general ServiceDown as it affects all monitoring
      - alert: PrometheusTargetDown
        expr: up{job=~"prometheus|node-exporter|cadvisor"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Monitoring target {{ $labels.job }} is down"
          description: "{{ $labels.job }} monitoring target on {{ $labels.instance }} has been down for more than 1 minute"

      # AdGuard Home DNS and ad-blocking service availability
      # Critical as network clients depend on this for DNS resolution
      # 1m threshold: DNS failures impact all network operations immediately
      - alert: AdGuardDown
        expr: up{job="adguard"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "AdGuard DNS service is down"
          description: "AdGuard service is not responding for more than 1 minute"

      # n8n workflow automation platform availability
      # Critical as automated workflows and integrations depend on this service
      # 2m threshold: Slightly longer than DNS as brief restarts for config changes are acceptable
      - alert: N8nDown
        expr: up{job="n8n"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "n8n automation service is down"
          description: "n8n service is not responding for more than 2 minutes"

      # Ollama local LLM inference API availability
      # Critical for AI-powered workflows and applications
      # 2m threshold: Tolerates brief delays during model loading operations
      - alert: OllamaDown
        expr: up{job="ollama"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Ollama AI service is down"
          description: "Ollama API is not responding for more than 2 minutes"

  - name: resource-alerts
    rules:
      # Disk I/O saturation monitoring
      # High I/O wait indicates disk bottleneck or potential hardware issues
      # >0.5 threshold: More than 50% of time spent in I/O operations indicates saturation
      # 10m duration: Tolerates brief I/O spikes from backups, large file ops, model downloads
      - alert: HighDiskIOWait
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High disk I/O wait time"
          description: "Disk I/O wait time is high on {{ $labels.instance }} for more than 10 minutes"

      # Network traffic anomaly detection
      # Monitors receive bandwidth on physical interfaces (excludes loopback and docker virtual interfaces)
      # >100MB/s threshold: Unusually high for typical home server traffic, may indicate abuse or attack
      # 10m duration: Allows for legitimate large transfers (Ollama models, backups)
      - alert: HighNetworkTraffic
        expr: rate(node_network_receive_bytes_total{device!~"lo|docker.*"}[5m]) > 100000000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High network traffic detected"
          description: "Network interface {{ $labels.device }} on {{ $labels.instance }} is receiving high traffic for more than 10 minutes"

      # System load average monitoring (normalized by CPU count)
      # Load average includes processes waiting for CPU and I/O
      # >2x CPU count threshold: Load of 2.0 per CPU indicates significant queuing/contention
      # 10m duration: Uses 15-min load avg, sustained high load indicates capacity issue
      - alert: SystemLoadHigh
        expr: node_load15 / count(node_cpu_seconds_total{mode="idle"}) without (cpu, mode) > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "System load is high"
          description: "15-minute load average on {{ $labels.instance }} is above 2x CPU count for more than 10 minutes"

  # VPN Monitoring Alerts
  # Monitors WireGuard VPN as the primary security boundary
  # Critical for maintaining secure remote access
  - name: vpn_alerts
    interval: 30s
    rules:
      # Monitor WireGuard service availability
      # This is critical as VPN is the primary remote access method
      - alert: WireGuardContainerDown
        expr: absent(container_last_seen{name="wireguard"}) or (time() - container_last_seen{name="wireguard"} > 120)
        for: 2m
        labels:
          severity: critical
          category: availability
          service: wireguard
        annotations:
          summary: "WireGuard VPN container is down"
          description: "WireGuard container has been down for more than 2 minutes. All remote VPN access is unavailable."

      # Monitor for excessive connection attempts (possible attack)
      # High rate of failed handshakes could indicate brute force or scanning
      - alert: WireGuardExcessiveConnectionAttempts
        expr: rate(container_network_receive_packets_total{name="wireguard"}[5m]) > 1000
        for: 3m
        labels:
          severity: warning
          category: security
          service: wireguard
        annotations:
          summary: "Unusual WireGuard connection activity"
          description: "High rate of network packets to WireGuard ({{ $value }} packets/sec). Possible port scanning or attack."

      # Monitor container restarts (could indicate crashes or issues)
      - alert: WireGuardFrequentRestarts
        expr: rate(container_start_time_seconds{name="wireguard"}[15m]) > 0.1
        for: 5m
        labels:
          severity: warning
          category: reliability
          service: wireguard
        annotations:
          summary: "WireGuard container restarting frequently"
          description: "WireGuard has restarted {{ $value }} times in 15 minutes. Check logs for errors."

      # Monitor container health status
      - alert: WireGuardUnhealthy
        expr: container_health_status{name="wireguard",status!="healthy"} == 1
        for: 5m
        labels:
          severity: warning
          category: reliability
          service: wireguard
        annotations:
          summary: "WireGuard container health check failing"
          description: "WireGuard health check has been failing for 5+ minutes."

  # Immich Monitoring Alerts
  # Self-hosted photo and video management with AI features
  - name: immich_alerts
    interval: 30s
    rules:
      # Monitor main Immich web service availability
      - alert: ImmichDown
        expr: absent(container_last_seen{name="immich-server"}) or (time() - container_last_seen{name="immich-server"} > 120)
        for: 2m
        labels:
          severity: critical
          category: availability
          service: immich
        annotations:
          summary: "Immich photo management service is down"
          description: "Immich server container has been down for more than 2 minutes. Photo upload and access unavailable."

      # Monitor Immich health check status
      - alert: ImmichUnhealthy
        expr: container_health_status{name="immich-server",status!="healthy"} == 1
        for: 5m
        labels:
          severity: warning
          category: reliability
          service: immich
        annotations:
          summary: "Immich container health check failing"
          description: "Immich health check has been failing for 5+ minutes. Check application logs."

      # Monitor PostgreSQL database availability
      - alert: ImmichDatabaseDown
        expr: absent(container_last_seen{name="immich-postgres"}) or (time() - container_last_seen{name="immich-postgres"} > 120)
        for: 2m
        labels:
          severity: critical
          category: availability
          service: immich
        annotations:
          summary: "Immich database is down"
          description: "PostgreSQL database for Immich has been down for more than 2 minutes. All photo operations will fail."

      # Monitor machine learning service for AI features
      - alert: ImmichMachineLearningDown
        expr: absent(container_last_seen{name="immich-machine-learning"}) or (time() - container_last_seen{name="immich-machine-learning"} > 120)
        for: 5m
        labels:
          severity: warning
          category: availability
          service: immich
        annotations:
          summary: "Immich machine learning service is down"
          description: "ML service down for 5+ minutes. Face recognition and object detection unavailable."

      # Monitor Redis cache availability
      - alert: ImmichRedisDown
        expr: absent(container_last_seen{name="immich-redis"}) or (time() - container_last_seen{name="immich-redis"} > 120)
        for: 2m
        labels:
          severity: critical
          category: availability
          service: immich
        annotations:
          summary: "Immich Redis cache is down"
          description: "Redis cache has been down for more than 2 minutes. Performance degradation expected."

      # Monitor for container restart loops
      - alert: ImmichFrequentRestarts
        expr: rate(container_start_time_seconds{name=~"immich-.*"}[15m]) > 0.1
        for: 5m
        labels:
          severity: warning
          category: reliability
          service: immich
        annotations:
          summary: "Immich container restarting frequently"
          description: "Container {{ $labels.name }} has restarted {{ $value }} times in 15 minutes. Check logs for errors."
