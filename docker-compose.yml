services:
  adguard:
    image: adguard/adguardhome:latest
    container_name: adguard-home
    restart: unless-stopped
    ports:
      - "${SERVER_IP}:53:53/tcp"
      - "${SERVER_IP}:53:53/udp"
      - "${SERVER_IP}:3000:3000/tcp"
      - "${SERVER_IP}:80:80/tcp"
    volumes:
      - ./data/adguard/work:/opt/adguardhome/work
      - ./data/adguard/conf:/opt/adguardhome/conf
    networks:
      - homeserver

  n8n-init:
    image: alpine:latest
    container_name: n8n-init
    restart: "no"
    volumes:
      - ./data/n8n:/data
    command: >
      sh -c "
        mkdir -p /data &&
        chown -R 1000:1000 /data &&
        chmod -R 755 /data
      "

  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    restart: unless-stopped
    user: "1000:1000"
    ports:
      - "${SERVER_IP}:5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_USER}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD}
      - N8N_HOST=${SERVER_IP}
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://${SERVER_IP}:5678/
      - GENERIC_TIMEZONE=${TIMEZONE}
      - N8N_SECURE_COOKIE=${N8N_SECURE_COOKIE}
    volumes:
      - ./data/n8n:/home/node/.n8n
    networks:
      - homeserver
    depends_on:
      - ollama
      - n8n-init

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "${SERVER_IP}:11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ./data/ollama:/root/.ollama
    networks:
      - homeserver

  ollama-setup:
    image: alpine/curl:latest
    container_name: ollama-setup
    restart: "no"
    depends_on:
      - ollama
    networks:
      - homeserver
    command: >
      sh -c "
        echo 'Waiting for Ollama to be ready...' &&
        sleep 15 &&
        echo 'Testing Ollama connectivity...' &&
        curl -f http://ollama:11434/api/version || (echo 'Ollama not ready, exiting' && exit 1) &&
        echo 'Pulling coding model: deepseek-coder-v2...' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"deepseek-coder-v2\"}' &&
        echo 'Pulling chat model: llama3.1:8b...' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"llama3.1:8b\"}' &&
        echo 'Models installation initiated successfully!'
      "

networks:
  homeserver:
    driver: bridge